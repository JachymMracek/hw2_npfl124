{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46b7310-79e9-4c19-8bf5-e75005ead756",
   "metadata": {},
   "source": [
    "# Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2019a5e8-8d9c-412a-bd33-c963584a12cb",
   "metadata": {},
   "source": [
    "## Name (as in SIS)\n",
    "Jáchym Mraček (mracekj1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67da1f3-31e5-4e2e-851a-7bfe719aa7f5",
   "metadata": {},
   "source": [
    "## Email address (as in SIS)\n",
    "jachym.mracek1010@gmail.com\n",
    "jachym.mracek777@student.cuni.cz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7123c2b7-48f5-4a29-86bc-444a40751fc9",
   "metadata": {},
   "source": [
    "## Code URL\n",
    "https://github.com/JachymMracek/hw2_npfl124\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f04618-c705-4ca3-b7f8-6928002762fe",
   "metadata": {},
   "source": [
    "## How did you implement tokenization and term normalization?\n",
    "tokenization - I used MosesTokenizer\n",
    "normalization - I deleted punctions,lowered tokens and lemmatizated tokens with simplemma. Each token must be only alphabetic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216dc99-c89e-45a5-8058-ebfa36a3a013",
   "metadata": {},
   "source": [
    "## Number of all tokens (after punctuation removal and term normalisation)\n",
    "Czech - 21732789\n",
    "English - 54667456\n",
    "\n",
    "together (english + czech)- 76 400 245"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4362e6-4bb3-4f19-b42f-745a9de4644b",
   "metadata": {},
   "source": [
    "## Number of all unique terms (size of the dictionary)\n",
    "Czech - 338320\n",
    "English - 354986"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d8f307-2819-4698-a1bd-26bfe6b304a0",
   "metadata": {},
   "source": [
    "## Number of all postings (lenght of all postings lists)\n",
    "Czech - 10737105\n",
    "English - 23939757"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd5f81-61b8-41a0-9003-3e4252528c06",
   "metadata": {},
   "source": [
    "## The term with the highest document frequency (longest postings list)\n",
    "Czech - v\n",
    "English - desk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8347e10-695d-4772-8208-79aa1ffe159b",
   "metadata": {},
   "source": [
    "## The highest document frequency (length of the longest postings list)\n",
    "Czech - 68894\n",
    "English - 87466"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb5649e-fc43-477a-bd88-0f05ddfab965",
   "metadata": {},
   "source": [
    "## The average document frequency (average length of the postings lists)\n",
    "Czech - 31.74\n",
    "English - 67.44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8576e4-7778-4d99-8e4c-a83f37c8c030",
   "metadata": {},
   "source": [
    "## English,Czech result file\n",
    "In this directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da15b9b2-3cea-4a39-8942-a8c873773482",
   "metadata": {},
   "source": [
    "# 10.2452/423-AH in English\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f48f70-cfa6-4d10-8f97-f3504e60285a",
   "metadata": {},
   "source": [
    "## Number of retrieved documents\n",
    "\n",
    "219"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6100a0a-3f21-4a4f-acc0-9abe9cb78067",
   "metadata": {},
   "source": [
    "## Number of retrieved documents that are relevant\n",
    "\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10bb55c-42ba-42ad-a019-d5cd4bf7718f",
   "metadata": {},
   "source": [
    "## Precission\n",
    "\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c551b4-b67d-48c6-8fd0-95be0ae7f80c",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e7820-6ac7-429b-869e-3427f655661b",
   "metadata": {},
   "source": [
    "# 10.2452/424-AH in English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007fe498-1537-4082-8a98-68d023a198e6",
   "metadata": {},
   "source": [
    "## Number of retrieved documents\n",
    "\n",
    "108"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa1f762-7d6e-4bc8-b9d8-f47aeb3d2926",
   "metadata": {},
   "source": [
    "## Number of retrieved documents that are relevant\n",
    "\n",
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542228ec-a44b-4803-b016-94168d46d36a",
   "metadata": {},
   "source": [
    "## Precission\n",
    "\n",
    "1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e341b-d1a7-4651-a2eb-b3665ae52d76",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a4c4e6-c421-4512-9354-96da40bab8c7",
   "metadata": {},
   "source": [
    "# 10.2452/425-AH in English\n",
    "\n",
    "207"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ecf1b-06d2-40b8-97eb-dda2b3e79778",
   "metadata": {},
   "source": [
    "## Number of retrieved documents\n",
    "\n",
    "16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae627add-d696-4a2d-bef0-59800a063671",
   "metadata": {},
   "source": [
    "## Precission\n",
    "\n",
    "7.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f17bb-3761-4c76-b635-55da6565f373",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5794e6a-2cce-44c8-8fff-033be22da1a0",
   "metadata": {},
   "source": [
    "## Average results over all queries in English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea76c9bc-7968-4637-baa0-bd943a2d2607",
   "metadata": {},
   "source": [
    "## Number of retrieved documents\n",
    "\n",
    "3.48120000e+02 = 348.12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8212b3-72f3-43a0-8067-dfdf26a163e3",
   "metadata": {},
   "source": [
    "## Number of retrieved documents that are relevant\n",
    "\n",
    "2.01600000e+01 = 20.16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3899626-d822-4fbd-bd91-d4a7e1f0cfa2",
   "metadata": {},
   "source": [
    "## Precision (in %)\n",
    "\n",
    "1.84062086e-01 = 18.4062086 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f09264e-9899-45bc-a201-bf7b0e5d2f2f",
   "metadata": {},
   "source": [
    "## Recall (in %)\n",
    "\n",
    "7.13238049e-01 = 71.3238049 %"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c27fd65-514f-4884-82a3-d7533065ffd4",
   "metadata": {},
   "source": [
    "# 10.2452/423-AH in Czech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af143c93-5fe0-4a08-9d4c-8819a5dc150d",
   "metadata": {},
   "source": [
    "## Number of retrieved documents\n",
    "\n",
    "116"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ff2cd-94d6-4c2b-8a22-17770f576aa4",
   "metadata": {},
   "source": [
    "## Number of retrieved documents that are relevant\n",
    "\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01723218-5b0e-4df8-bf84-ba968fabdefa",
   "metadata": {},
   "source": [
    "## Precission\n",
    "\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f502454c-7a8a-40b0-9cac-bfb616001b73",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f68f0-2c3b-4d59-b595-5102aa473389",
   "metadata": {},
   "source": [
    "# 10.2452/424-AH in Czech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c3eb7-c567-4617-8101-94bf3f90a059",
   "metadata": {},
   "source": [
    "## Number of retrieved documents\n",
    "\n",
    "21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b687b4-a623-491c-91a4-112afd2ef06e",
   "metadata": {},
   "source": [
    "## Number of retrieved documents that are relevant\n",
    "\n",
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fcb7cf-aeb3-42a4-8e33-09ad8ce4d03d",
   "metadata": {},
   "source": [
    "## Precision (in %)\n",
    "\n",
    "9.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef24c3d5-8ba2-4a32-8d4b-7d6f9be308f7",
   "metadata": {},
   "source": [
    "## Recall (in %)\n",
    "\n",
    "50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a154715-5e7a-448a-b109-a1a0959093e7",
   "metadata": {},
   "source": [
    "## 10.2452/425-AH in Czech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aba3fd-0bef-46fa-9a25-e8a618eb2111",
   "metadata": {},
   "source": [
    "## Number of retrieved documents\n",
    "\n",
    "1634"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f49ae6f-64a1-4f0f-bb04-625324aaa456",
   "metadata": {},
   "source": [
    "## Number of retrieved documents that are relevant\n",
    "\n",
    "9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b9544-33e2-463d-8ec4-3b80e599a152",
   "metadata": {},
   "source": [
    "## Precision (in %)\n",
    "\n",
    "0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be35dc-d4d7-4e39-a22e-bf23ad18158e",
   "metadata": {},
   "source": [
    "## Recall (in %)\n",
    "\n",
    "90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289dc718-10bb-4557-8239-f75f445c990a",
   "metadata": {},
   "source": [
    "# Average results over all queries in Czech\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d6a9bb-1610-4ef3-ad20-79441c0a86e0",
   "metadata": {},
   "source": [
    "## Number of retrieved documents\n",
    "\n",
    "159.32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be03a43-8d79-4379-8a44-2d0cadbc3ef0",
   "metadata": {},
   "source": [
    "## Number of retrieved documents that are relevant\n",
    "\n",
    "8.16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ae02c-f0c0-4121-bf25-7b8dbb06cda0",
   "metadata": {},
   "source": [
    "## Precision (in %)\n",
    "\n",
    "17.512624"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179086e-10fc-4551-9cec-6c3052033ca9",
   "metadata": {},
   "source": [
    "## Recall (in %)\n",
    "\n",
    "46.324341"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750bf194-b44b-447c-b79e-7fde5fda0c04",
   "metadata": {},
   "source": [
    "# Porovnání\n",
    "\n",
    "Průměrné počty dokumentů, jak relevantní nebo všech jsou větší u angličtiny, jelikož angličtina obsahuje daleko více dat. Precision je u aj,čj srovnatelná, ale u recall je u angličtiny větší."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11522877-306f-412c-a743-ff37787800bd",
   "metadata": {},
   "source": [
    "# Kod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b1dcb-3cda-47a4-a79c-c043ccb705cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from sacremoses import MosesPunctNormalizer,MosesTokenizer\n",
    "from abc import ABC, abstractmethod\n",
    "import string\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy\n",
    "import stanza\n",
    "import json \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import PorterStemmer\n",
    "import simplemma\n",
    "\n",
    "class Query:\n",
    "    def __init__(self,num,x,operation,y,qrels,result_name):\n",
    "        #  Zpracujeme dotaz a vypíšeme, odpovědi pro google formulář\n",
    "        self.documents = 0\n",
    "        self.relevant = 0\n",
    "        self.precision = 0\n",
    "        self.recall = 0\n",
    "\n",
    "        self.__eval(x,y,operation)\n",
    "        self.__get_relevant(num,qrels,result_name)\n",
    "        self.__get_precision_recall(num,qrels)\n",
    "\n",
    "        if num == \"10.2452/423-AH\" or num == \"10.2452/424-AH\" or num == \"10.2452/425-AH\":\n",
    "            print(f\"current number: {num}\")\n",
    "            print(f\"number of documents: {len(self.documents)}\")\n",
    "            print(f\"relevant documents: {self.relevant}\")\n",
    "            print(f\"precision: {self.precision}\")\n",
    "            print(f\"recall: {self.recall}\")\n",
    "    \n",
    "    def __eval(self,documents_x,documents_y,operation):\n",
    "        # Provedeme operaci\n",
    "        operator:Operator = None\n",
    "\n",
    "        if operation == \"AND\":\n",
    "            operator = And(documents_x)\n",
    "        \n",
    "        elif operation == \"OR\":\n",
    "            operator = Or(documents_x)\n",
    "\n",
    "        elif operation == \"AND NOT\":\n",
    "            operator = AndNot(documents_x)\n",
    "        \n",
    "        operator.function(documents_y)\n",
    "        \n",
    "        self.documents = operator.result_documents\n",
    "    \n",
    "    def __get_relevant(self,num,qrels:dict[str,set[str]],result_name):\n",
    "        used = set()\n",
    "        with open(result_name,\"w\") as result_file:\n",
    "            for document in sorted(self.documents):\n",
    "                if document in qrels[num] and document not in used:\n",
    "                    self.relevant += 1\n",
    "                    result_file.write(f\"{num} {document}\\n\")\n",
    "                    used.add(document)\n",
    "    \n",
    "    def __get_precision_recall(self,num,qrels):\n",
    "        self.precision = self.relevant / len(self.documents) if self.documents else 0\n",
    "        self.recall = self.relevant / len(qrels[num]) if qrels.get(num) else 0\n",
    "\n",
    "class Language(ABC):\n",
    "    def __init__(self,tag,documents,topics_train,result_filename,qrels_filename):\n",
    "        self.tag = tag\n",
    "        self.documents = documents\n",
    "        self.topics_train = topics_train\n",
    "        self.indexInvertor = IndexInvertor(tag)\n",
    "        self.result_filename = result_filename\n",
    "        self.qrels_filename = qrels_filename\n",
    "        self.queries:list[Query] = []\n",
    "        self.avarage_queries = None\n",
    "        self.tokens = 0 # počet tokenů, které byly zpracovány v dokumentech\n",
    "\n",
    "        if not os.path.exists(\"inverted_index____\" + self.tag + \".json\"): # Pokud soubor přečtených dokumentů existuje, pak nečteme znovu\n",
    "            self.__parse_documents() # zpracujeme dokumenty\n",
    "        \n",
    "        with open(\"inverted_index____\" + self.tag + \".json\", \"r\", encoding=\"utf-8\") as f: # Načteme přečtení dokumentů\n",
    "            inverted_index = json.load(f)\n",
    "\n",
    "        qrels = self.__parse_qrels() # Přečteme qrels soubor\n",
    "        self.__parse_queries(qrels,inverted_index) # přečteme soubor s queries (and,or,and not)\n",
    "        self.__get_average_queries() # ZÍskaneme průmerné hodnoty queries, které se po nás vyžadují\n",
    "\n",
    "    def get_tokens(self,text):\n",
    "        # Provedeme tokenizaci jazyka\n",
    "        tokenizer = MosesTokenizer(lang = self.tag)\n",
    "\n",
    "        return tokenizer.tokenize(text)\n",
    "    \n",
    "    def __get_average_queries(self):\n",
    "        # získaneme průměrnou hodnotu, přeš všechny queries\n",
    "        metrics = numpy.array([(len(queries.documents), queries.relevant, queries.precision, queries.recall) for queries in self.queries])\n",
    "        self.avarage_queries = numpy.mean(metrics, axis=0)\n",
    "\n",
    "    def __parse_qrels(self,RELEVANT = \"1\"):\n",
    "        # Zpracujeme  subor qrels, tak že do slovníku započítáme pouze relevantní soubory\n",
    "        qrels:dict[str,set[str]] = {}\n",
    "        with open(self.qrels_filename,\"r\") as result_file:\n",
    "            for line in result_file:\n",
    "                num, _, docid, relevant_num = line.strip().split(\" \")\n",
    "\n",
    "                if num not in qrels and relevant_num == RELEVANT:\n",
    "                    qrels[num] = {docid}\n",
    "                \n",
    "                elif num in qrels and relevant_num == RELEVANT:\n",
    "                    qrels[num].add(docid)\n",
    "                    \n",
    "        return qrels\n",
    "\n",
    "    def __parse_documents(self):\n",
    "        # Zpracujeme dokumenty\n",
    "        for xml_name in os.listdir(self.documents):\n",
    "            xml_path = os.path.join(self.documents, xml_name)\n",
    "\n",
    "            try: # Pokud soubor nejde načíst, je poškozený pak ho necháme být (teoreticky by šlo pracovat i s těmi rozbitými, ale předpokládám, že běžnou praxí je takové soubory \n",
    "                 # vynechat -> nevíme co v nich je a co je tam za chybu)\n",
    "\n",
    "                self.indexInvertor.read_xml(xml_path, self)\n",
    "            \n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Seřadíme hodnoty, jak bylo požadováno\n",
    "            self.indexInvertor.sort_values()\n",
    "\n",
    "        with open(\"inverted_index____\" + self.tag + \".json\", \"w\", encoding=\"utf-8\") as f: # Uložíme zpracované dokumenty do souboru\n",
    "            json.dump(self.indexInvertor.terms, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def __parse_queries(self,qrels,indexInvertor):\n",
    "        # Zpracujeme queries soubor\n",
    "        tree = ET.parse(self.topics_train)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for top in root.findall('top'):\n",
    "            documents_x = []\n",
    "            documents_y = []\n",
    "\n",
    "            num = top.findtext('num').strip()\n",
    "            query_text = top.findtext('query').strip()\n",
    "            parts = query_text.split(\" \")\n",
    "\n",
    "            if len(parts) == 3:\n",
    "                token_x, operator_tag, token_y = parts\n",
    "\n",
    "            elif len(parts) == 4 and parts[1]== \"AND\" and parts[2] == \"NOT\":\n",
    "                token_x = parts[0]\n",
    "                operator_tag = \"AND NOT\"\n",
    "                token_y = parts[3]\n",
    "\n",
    "            normalize_token_x = IndexInvertor(self.tag).normalize(token_x)\n",
    "            normalize_token_y = IndexInvertor(self.tag).normalize(token_y)\n",
    "\n",
    "            try:\n",
    "                documents_x = indexInvertor[normalize_token_x]\n",
    "                documents_y = indexInvertor[normalize_token_y]\n",
    "\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            query = Query(num,documents_x,operator_tag,documents_y,qrels,self.result_filename) # Zpracujeme queries\n",
    "            self.queries.append(query)\n",
    "    \n",
    "    def add_token(self):\n",
    "        # zpracovali jsme validní token\n",
    "        self.tokens += 1\n",
    "\n",
    "class Czech(Language):\n",
    "    # zdědíme od třídy jazyk a předáme naše české konstanty\n",
    "    def __init__(self, TAG = \"cs\",DOCUMENTS = \"documents_cs\",train =\"topics-train_cs.xml\",RESULT_FILENAME = \"results-cs.dat\",QRELS_TRAIN =\"qrels-train_cs.txt\"):\n",
    "        super().__init__(TAG,DOCUMENTS,train,RESULT_FILENAME,QRELS_TRAIN)\n",
    "\n",
    "class English(Language):\n",
    "    # zdědíme od třídy jazyk a předáme naše anglické konstanty\n",
    "    def __init__(self, TAG = \"en\",DOCUMENTS = \"documents_en\",train=\"topics-train_en.xml\",RESULT_FILENAME = \"results-en.dat\",QRELS_TRAIN = \"qrels-train_en.txt\"):\n",
    "        super().__init__(TAG,DOCUMENTS,train,RESULT_FILENAME,QRELS_TRAIN)\n",
    "\n",
    "class IndexInvertor:\n",
    "    def __init__(self,nlp):\n",
    "        self.nlp = nlp # spíše jazyk- tag\n",
    "        self.terms: dict[str, list[str]] = {} # termy: a unikátní soubory, kam patří\n",
    "\n",
    "    def __add(self, token: str, id_document: str):\n",
    "        # Přidáváme do slovníku, pokud se nejedná o duplikát\n",
    "        if token not in self.terms:\n",
    "            self.terms[token] = [id_document]\n",
    "        elif id_document not in self.terms[token]:\n",
    "            self.terms[token].append(id_document)\n",
    "\n",
    "    def normalize(self, word: str):\n",
    "        # Normalizujeme slovo, kde vyčistíme tečky a převedeme na malý znak\n",
    "        word_clean = ''.join(c.lower() for c in word if c not in string.punctuation) # Možná teoreticky nemusíme převádět bez teček a zachytit to až potom\n",
    "\n",
    "        if  len(word_clean) > 0: # pokud není null, abychom nedostlai chybu\n",
    "            return simplemma.lemmatize(word_clean, self.nlp)\n",
    "        \n",
    "        return \"\" # vrátímr nic a později vyloučíme, jako validní slovo\n",
    "    \n",
    "    def read_xml(self, xml_path: str, language: Language):\n",
    "        # Přečteme náš xml validní soubor\n",
    "\n",
    "        print(f\"reading: {xml_path}\")\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for doc in root.findall('DOC'): #Přečteme dokument\n",
    "            id_document = doc.findtext('DOCID')\n",
    "            texts = []\n",
    "\n",
    "            for descendant in doc.findall(\"*\"):\n",
    "                if descendant.tag == \"DOCID\" or descendant.tag == \"DOCNO\": # Nechceme id documentu \n",
    "                    continue\n",
    "\n",
    "                if descendant.text: # Jednáli se o text\n",
    "                    texts.append(descendant.text.strip())\n",
    "\n",
    "            completeText = \" \".join(texts) # Získaneme kompletní text dokumentu\n",
    "            tokens = language.get_tokens(completeText) # tokenizujeme\n",
    "\n",
    "            for token in tokens:\n",
    "                normalized_token = self.normalize(token) # normalizujeme\n",
    "\n",
    "                if normalized_token.isalpha(): # Poslední kontrola, pokud se jedná o validní slovo\n",
    "                    language.add_token() # přičteme jedničku -> přečetli jsme token\n",
    "                    self.__add(normalized_token, id_document) # Přidáme term do slovníku (pokud je unikátní)\n",
    "\n",
    "    def sort_values(self):\n",
    "        # seřadíme, jak bylo požadováno\n",
    "        for key in self.terms:\n",
    "            self.terms[key] = sorted(self.terms[key])\n",
    "\n",
    "class Operator(ABC):\n",
    "    def __init__(self,x):\n",
    "        self.result_documents = [] # Výsledné dokumenty operace\n",
    "        self.used = set(x) # set pro rychlejší práci\n",
    "\n",
    "    @abstractmethod\n",
    "    def function(self,y):\n",
    "        pass\n",
    "\n",
    "class And(Operator):\n",
    "    def __init__(self, x):\n",
    "        super().__init__(x)\n",
    "\n",
    "    def function(self,y):\n",
    "        # And -> Pokud je soubor v dokumentech x, pak operace je pravdivá\n",
    "        for document_y in y:\n",
    "            if document_y in self.used and document_y not in self.result_documents: # Nechceme duplikáty (i když nejspíše není možné aby byly, když už přidáváme pouze unikátní)\n",
    "                self.result_documents.append(document_y)\n",
    "    \n",
    "class Or(Operator):\n",
    "    def __init__(self, x):\n",
    "        super().__init__(x)\n",
    "\n",
    "    def function(self,y):\n",
    "        # Or -> všechny soubory v x jsou pravdivé a soubory v y, pouze pokud neobsahuje duplikát v x\n",
    "        for document_x in self.used:\n",
    "            self.result_documents.append(document_x)\n",
    "        \n",
    "        for document_y in y:\n",
    "            if document_y not in self.used and document_y not in self.result_documents:\n",
    "                self.result_documents.append(document_y)\n",
    "\n",
    "class AndNot(Operator):\n",
    "    def function(self,y):\n",
    "        # AndNot -> soubory v x nesmí být v y.\n",
    "        for document_x in self.used:\n",
    "            if document_x not in set(y): # Pro efektivitu dáváme y do množiny\n",
    "                self.result_documents.append(document_x)\n",
    "\n",
    "\n",
    "def google_task_questions(inverted_index:dict):\n",
    "    # Vyřešíme otázky v google dotazníku\n",
    "    number_of_unique_terms = len(inverted_index)\n",
    "    term_with_highest_document_frequency = \"\"\n",
    "    highest_document_frequency = 0\n",
    "    sum_posting_len = 0\n",
    "\n",
    "    for term, postings in inverted_index.items():\n",
    "        len_postings = len(postings)   \n",
    "\n",
    "        if len_postings > highest_document_frequency:\n",
    "            highest_document_frequency = len_postings\n",
    "            term_with_highest_document_frequency = term\n",
    "        \n",
    "        sum_posting_len += len_postings\n",
    "\n",
    "    average_document_frequency = round(sum_posting_len / number_of_unique_terms,2)\n",
    "\n",
    "    print(f\"number of unique terms: {number_of_unique_terms}\")\n",
    "    print(f\"sum of posting lengths: {sum_posting_len}\")\n",
    "    print(f\"term with highest document frequency: {term_with_highest_document_frequency}\")\n",
    "    print(f\"highest document frequency: {highest_document_frequency}\")\n",
    "    print(f\"highest document frequency: {highest_document_frequency}\")\n",
    "    print(f\"average document frequency: {average_document_frequency}\")\n",
    "\n",
    "def information_retrivial():\n",
    "    # Začneme hledat řešení, kde pro efektivitu si načteme zpracované soubory do jsonu, abychom lematizaci nemuseli dělat více krát.\n",
    "    languages:list[Language] = [Czech(),English()]\n",
    "\n",
    "    for language in languages:\n",
    "        print(f\"average queries: {language.avarage_queries}\")  # Vytiskneme průměrné hodnoty\n",
    "        print(f\"tokens: {language.tokens}\") # počet přečtených validních tokenů v souboru\n",
    "    \n",
    "    with open(\"inverted_index____\" + \"cs\" + \".json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            inverted_index_cs = json.load(f)\n",
    "    \n",
    "    with open(\"inverted_index____\" + \"en\" + \".json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            inverted_index_en = json.load(f)\n",
    "    \n",
    "    print(\"cz\")\n",
    "    google_task_questions(inverted_index_cs)\n",
    "    print(\"en\")\n",
    "    google_task_questions(inverted_index_en)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Spustíme řešení\n",
    "    information_retrivial()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
